{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gym"
      ],
      "metadata": {
        "id": "rvNVuqi-QVSD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "LOG_SIG_MAX = 2\n",
        "LOG_SIG_MIN = -20\n",
        "epsilon = 1e-6\n",
        "\n",
        "# Initialize Policy weights\n",
        "def weights_init_(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, hidden_dim):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        self.apply(weights_init_)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.linear1(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        # Q1 architecture\n",
        "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # Q2 architecture\n",
        "        self.linear4 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
        "        self.linear5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear6 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        self.apply(weights_init_)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        xu = torch.cat([state, action], 1)\n",
        "\n",
        "        x1 = F.relu(self.linear1(xu))\n",
        "        x1 = F.relu(self.linear2(x1))\n",
        "        x1 = self.linear3(x1)\n",
        "\n",
        "        x2 = F.relu(self.linear4(xu))\n",
        "        x2 = F.relu(self.linear5(x2))\n",
        "        x2 = self.linear6(x2)\n",
        "\n",
        "        return x1, x2\n",
        "\n",
        "\n",
        "class GaussianPolicy(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
        "        super(GaussianPolicy, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.mean_linear = nn.Linear(hidden_dim, num_actions)\n",
        "        self.log_std_linear = nn.Linear(hidden_dim, num_actions)\n",
        "\n",
        "        self.apply(weights_init_)\n",
        "\n",
        "        # action rescaling\n",
        "        if action_space is None:\n",
        "            self.action_scale = torch.tensor(1.)\n",
        "            self.action_bias = torch.tensor(0.)\n",
        "        else:\n",
        "            self.action_scale = torch.FloatTensor(\n",
        "                (action_space.high - action_space.low) / 2.)\n",
        "            self.action_bias = torch.FloatTensor(\n",
        "                (action_space.high + action_space.low) / 2.)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.linear1(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        mean = self.mean_linear(x)\n",
        "        log_std = self.log_std_linear(x)\n",
        "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
        "        return mean, log_std\n",
        "\n",
        "    def sample(self, state):\n",
        "        mean, log_std = self.forward(state)\n",
        "        std = log_std.exp()\n",
        "        normal = Normal(mean, std)\n",
        "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
        "        y_t = torch.tanh(x_t)\n",
        "        action = y_t * self.action_scale + self.action_bias\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "        # Enforcing Action Bound\n",
        "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + epsilon)\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\n",
        "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
        "        return action, log_prob, mean\n",
        "\n",
        "    def to(self, device):\n",
        "        self.action_scale = self.action_scale.to(device)\n",
        "        self.action_bias = self.action_bias.to(device)\n",
        "        return super(GaussianPolicy, self).to(device)\n",
        "\n",
        "\n",
        "class DeterministicPolicy(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
        "        super(DeterministicPolicy, self).__init__()\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.mean = nn.Linear(hidden_dim, num_actions)\n",
        "        self.noise = torch.Tensor(num_actions)\n",
        "\n",
        "        self.apply(weights_init_)\n",
        "\n",
        "        # action rescaling\n",
        "        if action_space is None:\n",
        "            self.action_scale = 1.\n",
        "            self.action_bias = 0.\n",
        "        else:\n",
        "            self.action_scale = torch.FloatTensor(\n",
        "                (action_space.high - action_space.low) / 2.)\n",
        "            self.action_bias = torch.FloatTensor(\n",
        "                (action_space.high + action_space.low) / 2.)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.linear1(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        mean = torch.tanh(self.mean(x)) * self.action_scale + self.action_bias\n",
        "        return mean\n",
        "\n",
        "    def sample(self, state):\n",
        "        mean = self.forward(state)\n",
        "        noise = self.noise.normal_(0., std=0.1)\n",
        "        noise = noise.clamp(-0.25, 0.25)\n",
        "        action = mean + noise\n",
        "        return action, torch.tensor(0.), mean\n",
        "\n",
        "    def to(self, device):\n",
        "        self.action_scale = self.action_scale.to(device)\n",
        "        self.action_bias = self.action_bias.to(device)\n",
        "        self.noise = self.noise.to(device)\n",
        "        return super(DeterministicPolicy, self).to(device)"
      ],
      "metadata": {
        "id": "W57WTqQVSvaF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "def create_log_gaussian(mean, log_std, t):\n",
        "    quadratic = -((0.5 * (t - mean) / (log_std.exp())).pow(2))\n",
        "    l = mean.shape\n",
        "    log_z = log_std\n",
        "    z = l[-1] * math.log(2 * math.pi)\n",
        "    log_p = quadratic.sum(dim=-1) - log_z.sum(dim=-1) - 0.5 * z\n",
        "    return log_p\n",
        "\n",
        "def logsumexp(inputs, dim=None, keepdim=False):\n",
        "    if dim is None:\n",
        "        inputs = inputs.view(-1)\n",
        "        dim = 0\n",
        "    s, _ = torch.max(inputs, dim=dim, keepdim=True)\n",
        "    outputs = s + (inputs - s).exp().sum(dim=dim, keepdim=True).log()\n",
        "    if not keepdim:\n",
        "        outputs = outputs.squeeze(dim)\n",
        "    return outputs\n",
        "\n",
        "def soft_update(target, source, tau):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
        "\n",
        "def hard_update(target, source):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(param.data)"
      ],
      "metadata": {
        "id": "Q_u7iSMtSs1D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity, seed):\n",
        "        random.seed(seed)\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def save_buffer(self, env_name, suffix=\"\", save_path=None):\n",
        "        if not os.path.exists('checkpoints/'):\n",
        "            os.makedirs('checkpoints/')\n",
        "\n",
        "        if save_path is None:\n",
        "            save_path = \"checkpoints/sac_buffer_{}_{}\".format(env_name, suffix)\n",
        "        print('Saving buffer to {}'.format(save_path))\n",
        "\n",
        "        with open(save_path, 'wb') as f:\n",
        "            pickle.dump(self.buffer, f)\n",
        "\n",
        "    def load_buffer(self, save_path):\n",
        "        print('Loading buffer from {}'.format(save_path))\n",
        "\n",
        "        with open(save_path, \"rb\") as f:\n",
        "            self.buffer = pickle.load(f)"
      ],
      "metadata": {
        "id": "jw00fhImS5Y1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "class SAC(object):\n",
        "    def __init__(self, num_inputs, action_space, args):\n",
        "\n",
        "        self.gamma = args.gamma\n",
        "        self.tau = args.tau\n",
        "        self.alpha = args.alpha\n",
        "\n",
        "        self.policy_type = args.policy\n",
        "        self.target_update_interval = args.target_update_interval\n",
        "        self.automatic_entropy_tuning = args.automatic_entropy_tuning\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "        self.critic = QNetwork(num_inputs, action_space.shape[0], args.hidden_size).to(device=self.device)\n",
        "        self.critic_optim = Adam(self.critic.parameters(), lr=args.lr)\n",
        "\n",
        "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], args.hidden_size).to(self.device)\n",
        "        hard_update(self.critic_target, self.critic)\n",
        "\n",
        "        if self.policy_type == \"Gaussian\":\n",
        "            # Target Entropy = −dim(A) (e.g. , -6 for HalfCheetah-v2) as given in the paper\n",
        "            if self.automatic_entropy_tuning is True:\n",
        "                self.target_entropy = -torch.prod(torch.Tensor(action_space.shape).to(self.device)).item()\n",
        "                self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
        "                self.alpha_optim = Adam([self.log_alpha], lr=args.lr)\n",
        "\n",
        "            self.policy = GaussianPolicy(num_inputs, action_space.shape[0], args.hidden_size, action_space).to(self.device)\n",
        "            self.policy_optim = Adam(self.policy.parameters(), lr=args.lr)\n",
        "\n",
        "        else:\n",
        "            self.alpha = 0\n",
        "            self.automatic_entropy_tuning = False\n",
        "            self.policy = DeterministicPolicy(num_inputs, action_space.shape[0], args.hidden_size, action_space).to(self.device)\n",
        "            self.policy_optim = Adam(self.policy.parameters(), lr=args.lr)\n",
        "\n",
        "    def select_action(self, state, evaluate=False):\n",
        "        state = torch.FloatTensor(np.array(state)).to(self.device).unsqueeze(0)\n",
        "\n",
        "        if evaluate is False:\n",
        "            action, _, _ = self.policy.sample(state)\n",
        "        else:\n",
        "            _, _, action = self.policy.sample(state)\n",
        "        return action.detach().cpu().numpy()[0]\n",
        "\n",
        "    def update_parameters(self, memory, batch_size, updates):\n",
        "        # Sample a batch from memory\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size=batch_size)\n",
        "\n",
        "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
        "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
        "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
        "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
        "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_state_action, next_state_log_pi, _ = self.policy.sample(next_state_batch)\n",
        "            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_action)\n",
        "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
        "            next_q_value = reward_batch + mask_batch * self.gamma * (min_qf_next_target)\n",
        "        qf1, qf2 = self.critic(state_batch, action_batch)  # Two Q-functions to mitigate positive bias in the policy improvement step\n",
        "        qf1_loss = F.mse_loss(qf1, next_q_value)  # JQ = 𝔼(st,at)~D[0.5(Q1(st,at) - r(st,at) - γ(𝔼st+1~p[V(st+1)]))^2]\n",
        "        qf2_loss = F.mse_loss(qf2, next_q_value)  # JQ = 𝔼(st,at)~D[0.5(Q1(st,at) - r(st,at) - γ(𝔼st+1~p[V(st+1)]))^2]\n",
        "        qf_loss = qf1_loss + qf2_loss\n",
        "\n",
        "        self.critic_optim.zero_grad()\n",
        "        qf_loss.backward()\n",
        "        self.critic_optim.step()\n",
        "\n",
        "        pi, log_pi, _ = self.policy.sample(state_batch)\n",
        "\n",
        "        qf1_pi, qf2_pi = self.critic(state_batch, pi)\n",
        "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
        "\n",
        "        policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean() # Jπ = 𝔼st∼D,εt∼N[α * logπ(f(εt;st)|st) − Q(st,f(εt;st))]\n",
        "\n",
        "        self.policy_optim.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.policy_optim.step()\n",
        "\n",
        "        if self.automatic_entropy_tuning:\n",
        "            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
        "\n",
        "            self.alpha_optim.zero_grad()\n",
        "            alpha_loss.backward()\n",
        "            self.alpha_optim.step()\n",
        "\n",
        "            self.alpha = self.log_alpha.exp()\n",
        "            alpha_tlogs = self.alpha.clone() # For TensorboardX logs\n",
        "        else:\n",
        "            alpha_loss = torch.tensor(0.).to(self.device)\n",
        "            alpha_tlogs = torch.tensor(self.alpha) # For TensorboardX logs\n",
        "\n",
        "\n",
        "        if updates % self.target_update_interval == 0:\n",
        "            soft_update(self.critic_target, self.critic, self.tau)\n",
        "\n",
        "        return qf1_loss.item(), qf2_loss.item(), policy_loss.item(), alpha_loss.item(), alpha_tlogs.item()\n",
        "\n",
        "    # Save model parameters\n",
        "    def save_checkpoint(self, env_name, suffix=\"\", ckpt_path=None):\n",
        "        if not os.path.exists('checkpoints/'):\n",
        "            os.makedirs('checkpoints/')\n",
        "        if ckpt_path is None:\n",
        "            ckpt_path = \"checkpoints/sac_checkpoint_{}_{}\".format(env_name, suffix)\n",
        "        print('Saving models to {}'.format(ckpt_path))\n",
        "        torch.save({'policy_state_dict': self.policy.state_dict(),\n",
        "                    'critic_state_dict': self.critic.state_dict(),\n",
        "                    'critic_target_state_dict': self.critic_target.state_dict(),\n",
        "                    'critic_optimizer_state_dict': self.critic_optim.state_dict(),\n",
        "                    'policy_optimizer_state_dict': self.policy_optim.state_dict()}, ckpt_path)\n",
        "\n",
        "    # Load model parameters\n",
        "    def load_checkpoint(self, ckpt_path, evaluate=False):\n",
        "        print('Loading models from {}'.format(ckpt_path))\n",
        "        if ckpt_path is not None:\n",
        "            checkpoint = torch.load(ckpt_path)\n",
        "            self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "            self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
        "            self.critic_target.load_state_dict(checkpoint['critic_target_state_dict'])\n",
        "            self.critic_optim.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
        "            self.policy_optim.load_state_dict(checkpoint['policy_optimizer_state_dict'])\n",
        "\n",
        "            if evaluate:\n",
        "                self.policy.eval()\n",
        "                self.critic.eval()\n",
        "                self.critic_target.eval()\n",
        "            else:\n",
        "                self.policy.train()\n",
        "                self.critic.train()\n",
        "                self.critic_target.train()\n"
      ],
      "metadata": {
        "id": "cBONcGjcSf5M"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from types import SimpleNamespace\n",
        "\n",
        "# --- Настройка среды и параметров ---\n",
        "ENV_NAME = \"Pendulum-v1\"\n",
        "env = gym.make(ENV_NAME)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_space = env.action_space\n",
        "\n",
        "args = SimpleNamespace(\n",
        "    gamma=0.99,\n",
        "    tau=0.005,\n",
        "    alpha=0.2,\n",
        "    policy=\"Gaussian\",\n",
        "    target_update_interval=1,\n",
        "    automatic_entropy_tuning=True,\n",
        "    hidden_size=256,\n",
        "    lr=3e-4,\n",
        "    cuda=False\n",
        ")\n",
        "\n",
        "agent = SAC(state_dim, action_space, args)\n",
        "\n",
        "# --- Буфер воспроизведения ---\n",
        "replay_buffer = ReplayBuffer(1_000_000)\n",
        "\n",
        "# --- Параметры обучения ---\n",
        "MAX_EPISODES = 300\n",
        "MAX_STEPS = 200\n",
        "BATCH_SIZE = 256\n",
        "WARMUP_STEPS = 1000\n",
        "UPDATE_PER_STEP = 1\n",
        "RENDER_INTERVAL = 50\n",
        "\n",
        "returns = []\n",
        "\n",
        "# --- Обучение ---\n",
        "total_steps = 0\n",
        "for episode in range(MAX_EPISODES):\n",
        "    state,_ = env.reset()\n",
        "    episode_reward = 0\n",
        "\n",
        "    for step in range(MAX_STEPS):\n",
        "        if total_steps < WARMUP_STEPS:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = agent.select_action(state)\n",
        "\n",
        "        next_state, reward, terminated, truncated = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        replay_buffer(state, action, reward, next_state, float(not done))\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        total_steps += 1\n",
        "\n",
        "        if len(replay_buffer) > BATCH_SIZE:\n",
        "            for _ in range(UPDATE_PER_STEP):\n",
        "                agent.update_parameters(replay_buffer, BATCH_SIZE, total_steps)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    returns.append(episode_reward)\n",
        "    print(f\"Episode {episode}, Reward: {episode_reward:.2f}\")\n",
        "\n",
        "    # --- Запись видео ---\n",
        "    if episode % RENDER_INTERVAL == 0:\n",
        "        frames = []\n",
        "        state, _ = env.reset()\n",
        "        for _ in range(200):\n",
        "            frame = env.render(mode=\"rgb_array\")\n",
        "            frames.append(frame)\n",
        "            action = agent.select_action(state, evaluate=True)\n",
        "            state, _, terminated, truncated, _ = env.step(action)\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        video_path = f\"./sac_video_ep{episode}.mp4\"\n",
        "        imageio.mimsave(video_path, frames, fps=30)\n",
        "        print(f\"Saved video: {video_path}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "# --- График ---\n",
        "plt.plot(returns)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Soft Actor-Critic on Pendulum\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "WcEJucTdTeFu",
        "outputId": "6c9842e3-0bb4-451f-faf2-52e27a342c84"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'ReplayBuffer' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-3378363343.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'ReplayBuffer' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from types import SimpleNamespace\n",
        "\n",
        "# --- Параметры ---\n",
        "env_name = \"Pendulum-v1\"\n",
        "video_dir = \"./sac_videos\"\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "args = SimpleNamespace(\n",
        "    gamma=0.99,\n",
        "    tau=0.005,\n",
        "    alpha=0.2,\n",
        "    policy=\"Gaussian\",\n",
        "    target_update_interval=1,\n",
        "    automatic_entropy_tuning=True,\n",
        "    hidden_size=256,\n",
        "    lr=3e-4,\n",
        "    cuda=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# --- Инициализация ---\n",
        "env = gym.make(env_name)\n",
        "env.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "\n",
        "agent = SAC(env.observation_space.shape[0], env.action_space, args)\n",
        "replay_buffer = ReplayMemory(capacity=1_000_000, seed=args.seed)\n",
        "\n",
        "MAX_EPISODES = 300\n",
        "MAX_STEPS = 200\n",
        "BATCH_SIZE = 256\n",
        "START_STEPS = 1000\n",
        "UPDATES_PER_STEP = 1\n",
        "REWARD_HISTORY = []\n",
        "\n",
        "# --- Видео запись ---\n",
        "\"\"\"def record_video(agent, env_name, episode, duration=8):\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    state,_ = env.reset()\n",
        "    frames = []\n",
        "    done = False\n",
        "    t = 0\n",
        "    max_frames = int(30 * duration)  # 30 fps\n",
        "\n",
        "    while not done and t < max_frames:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "\n",
        "        action = agent.select_action(state, evaluate=True)\n",
        "        next_state, _, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        state = next_state\n",
        "        t += 1\n",
        "\n",
        "    video_path = os.path.join(video_dir, f\"sac_ep_{episode}.mp4\")\n",
        "    imageio.mimsave(video_path, frames, fps=30)\n",
        "    print(f\"[Video] Saved to {video_path}\")\n",
        "    env.close()\"\"\"\n",
        "\n",
        "# --- Обучение ---\n",
        "for episode in range(MAX_EPISODES):\n",
        "    state = env.reset()[0]\n",
        "    episode_reward = 0\n",
        "    episode_steps = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if len(replay_buffer) < START_STEPS:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = agent.select_action(state)\n",
        "\n",
        "        next_state, reward, terminated, truncated = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        mask = 0.0 if done else 1.0\n",
        "\n",
        "        replay_buffer.push(\n",
        "        np.array(state, dtype=np.float32),\n",
        "        np.array(action, dtype=np.float32),\n",
        "        reward,\n",
        "        np.array(next_state, dtype=np.float32),\n",
        "        mask\n",
        "    )\n",
        "        print(replay_buffer)\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        episode_steps += 1\n",
        "\n",
        "        if len(replay_buffer) >= BATCH_SIZE:\n",
        "            for _ in range(UPDATES_PER_STEP):\n",
        "                agent.update_parameters(replay_buffer, BATCH_SIZE, episode_steps)\n",
        "\n",
        "    REWARD_HISTORY.append(episode_reward)\n",
        "    print(f\"[Episode {episode}] Reward: {episode_reward:.2f}\")\n",
        "\n",
        "    if episode % 50 == 0:\n",
        "       pass #record_video(agent, env_name, episode)\n",
        "\n",
        "# --- График ---\n",
        "plt.plot(REWARD_HISTORY)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"SAC on Pendulum-v1\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cFpLUx00WfRd",
        "outputId": "19cedeba-cf32-4766-b20e-542a91855a83"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "[Episode 0] Reward: -1461.53\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n",
            "<__main__.ReplayMemory object at 0x7be12adb9610>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "all input arrays must have the same shape",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-31-3226847092.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUPDATES_PER_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mREWARD_HISTORY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-21-2735031436.py\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, memory, batch_size, updates)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Sample a batch from memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mstate_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-10-643635409.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
          ]
        }
      ]
    }
  ]
}