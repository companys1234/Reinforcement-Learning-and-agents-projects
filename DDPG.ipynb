{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "FyR1ON_WQZ3s",
        "outputId": "9ce487cd-3687-4df0-9e9c-8befdf33804b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.5.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.8.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "2c0c8d4fbe734d20bfea318ff89af9f8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "\n",
        "# --- Настройки ---\n",
        "ENV_NAME = \"Pendulum-v1\"\n",
        "GAMMA = 0.99\n",
        "TAU = 0.005\n",
        "ACTOR_LR = 1e-4\n",
        "CRITIC_LR = 1e-3\n",
        "BUFFER_SIZE = int(1e6)\n",
        "BATCH_SIZE = 128\n",
        "EPISODES = 300\n",
        "EXPL_NOISE = 0.1\n",
        "VIDEO_INTERVAL = 50\n",
        "SAVE_DIR = \"./ddpg_videos\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# --- Сеть Актор ---\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 400), nn.ReLU(),\n",
        "            nn.Linear(400, 300), nn.ReLU(),\n",
        "            nn.Linear(300, action_dim), nn.Tanh()\n",
        "        )\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.net(state) * self.max_action\n",
        "\n",
        "# --- Сеть Критик ---\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim, 400), nn.ReLU(),\n",
        "            nn.Linear(400, 300), nn.ReLU(),\n",
        "            nn.Linear(300, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        return self.net(torch.cat([state, action], 1))\n",
        "\n",
        "# --- Буфер повторов ---\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=BUFFER_SIZE):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = zip(*batch)\n",
        "        return (\n",
        "            torch.tensor(state, dtype=torch.float32),\n",
        "            torch.tensor(action, dtype=torch.float32),\n",
        "            torch.tensor(reward, dtype=torch.float32).unsqueeze(1),\n",
        "            torch.tensor(next_state, dtype=torch.float32),\n",
        "            torch.tensor(done, dtype=torch.float32).unsqueeze(1)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# --- Копирование весов с коэффициентом TAU ---\n",
        "def soft_update(target, source):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(target_param.data * (1.0 - TAU) + param.data * TAU)\n",
        "\n",
        "# --- Видео ---\n",
        "def record_video(env, actor, episode, max_action):\n",
        "    frames = []\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    step = 0\n",
        "    max_steps = 300  # ~8 секунд при fps=30\n",
        "\n",
        "    while not done and step < max_steps:\n",
        "        frame = env.render(mode=\"rgb_array\")\n",
        "        frames.append(frame)\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            action = actor(state_tensor).cpu().numpy()[0]\n",
        "        state, _, done, _, _ = env.step(action)\n",
        "        step += 1\n",
        "\n",
        "    filename = os.path.join(SAVE_DIR, f\"ddpg_ep_{episode}.mp4\")\n",
        "    imageio.mimsave(filename, frames, fps=30)\n",
        "    print(f\"Saved video: {filename}\")\n",
        "\n",
        "# --- Тренировка DDPG ---\n",
        "def train():\n",
        "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    actor_target.load_state_dict(actor.state_dict())\n",
        "\n",
        "    critic = Critic(state_dim, action_dim).to(device)\n",
        "    critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    critic_target.load_state_dict(critic.state_dict())\n",
        "\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
        "\n",
        "    replay_buffer = ReplayBuffer()\n",
        "    rewards_history = []\n",
        "\n",
        "    for episode in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                action = actor(state_tensor).cpu().numpy()[0]\n",
        "            action = (action + np.random.normal(0, EXPL_NOISE, size=action_dim)).clip(-max_action, max_action)\n",
        "\n",
        "            next_state, reward, terminated, truncated = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            print(done)\n",
        "            # Добавляем данные в буфер\n",
        "            replay_buffer.add((state, action, reward, next_state, float(done[0])))\n",
        "\n",
        "\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if len(replay_buffer) > BATCH_SIZE:\n",
        "                s, a, r, s2, d = replay_buffer.sample(BATCH_SIZE)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    target_q = critic_target(s2, actor_target(s2))\n",
        "                    target = r + (1 - d) * GAMMA * target_q\n",
        "\n",
        "                current_q = critic(s, a)\n",
        "                critic_loss = nn.MSELoss()(current_q, target)\n",
        "\n",
        "                critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                critic_optimizer.step()\n",
        "\n",
        "                actor_loss = -critic(s, actor(s)).mean()\n",
        "\n",
        "                actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                actor_optimizer.step()\n",
        "\n",
        "                soft_update(actor_target, actor)\n",
        "                soft_update(critic_target, critic)\n",
        "\n",
        "        rewards_history.append(episode_reward)\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode}, Reward: {episode_reward:.1f}\")\n",
        "\n",
        "        if episode % VIDEO_INTERVAL == 0:\n",
        "            record_video(env, actor, episode, max_action)\n",
        "\n",
        "    env.close()\n",
        "    plt.plot(rewards_history)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.title(\"DDPG on \" + ENV_NAME)\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "GYAG3TUGVWxl",
        "outputId": "0431a908-b0a8-4563-866b-f6d91e4dd2a0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-22-2065662555.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-22-2065662555.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m# Добавляем данные в буфер\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === Нейросети Actor и Critic ===\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 400)\n",
        "        self.fc2 = nn.Linear(400, 300)\n",
        "        self.fc3 = nn.Linear(300, action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x)) * self.max_action\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.fc2 = nn.Linear(400, 300)\n",
        "        self.fc3 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        x = F.relu(self.fc1(torch.cat([x, u], 1)))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# === Буфер опыта ===\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=1e6):\n",
        "        self.buffer = deque(maxlen=int(max_size))\n",
        "\n",
        "    def add(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        return (\n",
        "            torch.FloatTensor(np.array(states)).to(device),\n",
        "            torch.FloatTensor(np.array(actions)).to(device),\n",
        "            torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(device),\n",
        "            torch.FloatTensor(np.array(next_states)).to(device),\n",
        "            torch.FloatTensor(np.array(dones)).unsqueeze(1).to(device)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# === DDPG Агент ===\n",
        "class DDPGAgent:\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.noise_std = 0.2\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005\n",
        "\n",
        "    def select_action(self, state, noise=True):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        with torch.no_grad():\n",
        "            action = self.actor(state).cpu().numpy().flatten()\n",
        "        if noise:\n",
        "            action = (action + np.random.normal(0, self.noise_std, size=action.shape)).clip(-self.max_action, self.max_action)\n",
        "        return action\n",
        "\n",
        "    def update(self, replay_buffer, batch_size=64):\n",
        "        if len(replay_buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        # Сэмплируем батч\n",
        "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "        # Вычисляем целевое Q значение\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.actor_target(next_states)\n",
        "            target_Q = self.critic_target(next_states, next_actions)\n",
        "            target_Q = rewards + (1 - dones) * self.gamma * target_Q\n",
        "\n",
        "        # Обучаем Critic\n",
        "        current_Q = self.critic(states, actions)\n",
        "        critic_loss = F.mse_loss(current_Q, target_Q)\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Обучаем Actor\n",
        "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Обновляем целевые сети\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "    def save(self, filename):\n",
        "        torch.save(self.actor.state_dict(), f\"{filename}_actor.pth\")\n",
        "        torch.save(self.critic.state_dict(), f\"{filename}_critic.pth\")\n",
        "\n",
        "    def load(self, filename):\n",
        "        self.actor.load_state_dict(torch.load(f\"{filename}_actor.pth\", map_location=device))\n",
        "        self.critic.load_state_dict(torch.load(f\"{filename}_critic.pth\", map_location=device))\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "\n",
        "# === Функция тестирования ===\n",
        "def test_agent(env, agent, episodes=3):\n",
        "    for _ in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.select_action(state, noise=False)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "        print(f\"Test episode reward: {total_reward}\")\n",
        "\n",
        "\n",
        "# === Обучение ===\n",
        "# === Обучение ===\n",
        "def train_ddpg():\n",
        "    env_name = 'Pendulum-v1'\n",
        "    env = gym.make(env_name, render_mode=\"human\")\n",
        "    eval_env = gym.make(env_name, render_mode=\"human\")\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    agent = DDPGAgent(state_dim, action_dim, max_action)\n",
        "    replay_buffer = ReplayBuffer()\n",
        "\n",
        "    total_timesteps = 0\n",
        "    max_timesteps = 100000\n",
        "    start_timesteps = 10000\n",
        "\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num = 0\n",
        "\n",
        "    while total_timesteps < max_timesteps:\n",
        "        episode_timesteps += 1\n",
        "        total_timesteps += 1\n",
        "\n",
        "        # Выбираем действие\n",
        "        if total_timesteps < start_timesteps:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = agent.select_action(state)\n",
        "\n",
        "        # Делаем шаг в среде\n",
        "        next_state, reward, terminated, truncated = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Сохраняем в буфер\n",
        "        replay_buffer.add((state, action, reward, next_state, (done)))\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Если эпизод закончен\n",
        "        if done:\n",
        "            print(f\"Total T: {total_timesteps} Episode Num: {episode_num + 1} Episode T: {episode_timesteps} Reward: {episode_reward:.2f}\")\n",
        "            state, _ = env.reset()\n",
        "            episode_reward = 0\n",
        "            episode_timesteps = 0\n",
        "            episode_num += 1\n",
        "\n",
        "        # Обновляем политику\n",
        "        agent.update(replay_buffer)\n",
        "\n",
        "        # Тестирование каждые 20 эпизодов\n",
        "        if (episode_num % 20 == 0) and (episode_num > 0):\n",
        "            test_agent(eval_env, agent)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_ddpg()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "SUygkkSta_28",
        "outputId": "f86241b5-6874-4932-b7c0-1e12b3ad7297"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-35-4257370228.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mtrain_ddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-35-4257370228.py\u001b[0m in \u001b[0;36mtrain_ddpg\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;31m# Обновляем политику\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;31m# Тестирование каждые 20 эпизодов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-35-4257370228.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, replay_buffer, batch_size)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Сэмплируем батч\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Вычисляем целевое Q значение\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-35-4257370228.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
          ]
        }
      ]
    }
  ]
}