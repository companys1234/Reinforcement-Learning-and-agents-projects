{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "Ng7C4EC69CYB",
        "outputId": "84a520af-f9e6-409a-aab8-1a84d311b2fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.5.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.8.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "dd88a323fee04aa39b42d45f1d377345"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === Actor-Critic модель ===\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(num_inputs, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, num_actions)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(num_inputs, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        value = self.critic(x)\n",
        "        logits = self.actor(x)\n",
        "        return logits, value\n",
        "\n",
        "    def get_distribution(self, x):\n",
        "        logits, _ = self.forward(x)\n",
        "        return Categorical(logits=logits)\n",
        "\n",
        "    def act(self, state):\n",
        "        dist, value = self.forward(state)\n",
        "        action = Categorical(logits=dist).sample()\n",
        "        return action, value\n",
        "\n",
        "# === Вспомогательные функции ===\n",
        "def flat_params(model):\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        params.append(param.view(-1))\n",
        "    return torch.cat(params)\n",
        "\n",
        "def assign_params(model, flat_params):\n",
        "    prev_ind = 0\n",
        "    for param in model.parameters():\n",
        "        flat_size = int(np.prod(list(param.size())))\n",
        "        param.data.copy_(\n",
        "            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
        "        prev_ind += flat_size\n",
        "\n",
        "def conjugate_gradient(fvp_fun, b_vec, nsteps, residual_tol=1e-10):\n",
        "    x = torch.zeros_like(b_vec)\n",
        "    r = b_vec.clone()\n",
        "    p = r.clone()\n",
        "    r_dot_r = torch.dot(r, r)\n",
        "    for i in range(nsteps):\n",
        "        Ap = fvp_fun(p)\n",
        "        alpha = r_dot_r / torch.dot(p, Ap)\n",
        "        x += alpha * p\n",
        "        r -= alpha * Ap\n",
        "        r_dot_r_new = torch.dot(r, r)\n",
        "        if r_dot_r_new < residual_tol:\n",
        "            break\n",
        "        beta = r_dot_r_new / r_dot_r\n",
        "        p = r + beta * p\n",
        "        r_dot_r = r_dot_r_new\n",
        "    return x\n",
        "\n",
        "def fisher_vector_product(states, old_dist, model, damping=0.1):\n",
        "    def FVP(v):\n",
        "        kl = 0\n",
        "        for state in states:\n",
        "            new_dist = model.get_distribution(state)\n",
        "            kl += torch.mean(torch.distributions.kl.kl_divergence(old_dist, new_dist))\n",
        "        grads = torch.autograd.grad(kl, model.actor.parameters(), create_graph=True)\n",
        "        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
        "        grad_kl_v = torch.dot(flat_grad_kl, v)\n",
        "        grads2 = torch.autograd.grad(grad_kl_v, model.actor.parameters())\n",
        "        flat_grads2 = torch.cat([grad.contiguous().view(-1) for grad in grads2])\n",
        "        return flat_grads2 + damping * v\n",
        "    return FVP\n",
        "\n",
        "# === Сбор траекторий ===\n",
        "def collect_trajectories(env, model, horizon=1000):\n",
        "    states, actions, rewards, values, log_probs, masks = [], [], [], [], [], []\n",
        "\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    for step in range(horizon):\n",
        "        state_tensor = torch.FloatTensor(state).to(device)\n",
        "        with torch.no_grad():\n",
        "            dist, value = model(state_tensor)\n",
        "        action = Categorical(logits=dist).sample()\n",
        "        log_prob = Categorical(logits=dist).log_prob(action)\n",
        "\n",
        "        next_state, reward, terminated, truncated = env.step(action.item())\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Сохраняем данные\n",
        "        states.append(state_tensor)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        values.append(value)\n",
        "        log_probs.append(log_prob)\n",
        "        masks.append(not done)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Последнее значение для GAE\n",
        "    with torch.no_grad():\n",
        "        next_value = model(torch.FloatTensor(next_state).to(device))[1]\n",
        "\n",
        "    returns = discount_rewards(torch.tensor(rewards + [next_value.item()], device=device), gamma=0.99)[:-1]\n",
        "    advantages = returns - torch.stack(values).squeeze()\n",
        "\n",
        "    return (\n",
        "        torch.stack(states),\n",
        "        torch.stack(actions),\n",
        "        torch.tensor(rewards, device=device),\n",
        "        returns,\n",
        "        torch.stack(log_probs),\n",
        "        torch.tensor(masks, dtype=torch.float32, device=device),\n",
        "        total_reward\n",
        "    )\n",
        "\n",
        "def discount_rewards(rewards, gamma):\n",
        "    R = 0\n",
        "    returns = []\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    return torch.tensor(returns, device=device)\n",
        "\n",
        "# === Обновление TRPO ===\n",
        "def trpo_update(model, states, actions, old_log_probs, advantages, max_kl=0.01, cg_iters=10, damping=0.1):\n",
        "    logits = model.actor(states)\n",
        "    dist = Categorical(logits=logits)\n",
        "    log_probs = dist.log_prob(actions)\n",
        "    ratio = torch.exp(log_probs - old_log_probs)\n",
        "\n",
        "    loss = -(ratio * advantages).mean()\n",
        "\n",
        "    grads = torch.autograd.grad(loss, model.actor.parameters())\n",
        "    flat_grad = torch.cat([grad.view(-1) for grad in grads])\n",
        "\n",
        "    # Получаем FVP\n",
        "    old_dist = Categorical(logits=logits.detach())\n",
        "    fvp_fun = fisher_vector_product(states, old_dist, model, damping=damping)\n",
        "\n",
        "    step_dir = conjugate_gradient(lambda v: fvp_fun(v), flat_grad, cg_iters)\n",
        "\n",
        "    shs = 0.5 * torch.dot(step_dir, fvp_fun(step_dir))\n",
        "    lm = torch.sqrt(max_kl / shs)\n",
        "    full_step = step_dir * lm\n",
        "\n",
        "    # Выполняем line search\n",
        "    prev_params = flat_params(model.actor)\n",
        "    success = False\n",
        "    for j in range(10):\n",
        "        new_params = prev_params - full_step * (0.5 ** j)\n",
        "        assign_params(model.actor, new_params)\n",
        "\n",
        "        logits_new = model.actor(states)\n",
        "        if torch.isnan(logits_new).any():\n",
        "          print(\"NaN detected in logits. Skipping update.\")\n",
        "          assign_params(model.actor, prev_params)\n",
        "          return\n",
        "        new_dist = Categorical(logits=logits_new)\n",
        "        kl = torch.mean(torch.distributions.kl.kl_divergence(old_dist, new_dist))\n",
        "\n",
        "        if kl <= max_kl:\n",
        "            success = True\n",
        "            break\n",
        "        assign_params(model.actor, prev_params)\n",
        "\n",
        "    # Обновляем critic\n",
        "    values = model.critic(states).squeeze()\n",
        "    critic_loss = F.mse_loss(values, advantages + values.detach())\n",
        "    model.critic.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    model.critic_optimizer.step()\n",
        "\n",
        "# === Тестирование модели ===\n",
        "def test_model(env, model, episodes=3):\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                logits, _ = model(torch.FloatTensor(state).to(device))\n",
        "                dist = Categorical(logits=logits)\n",
        "                action = dist.sample()\n",
        "            next_state, reward, terminated, truncated = env.step(action.item())\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "        print(f\"Test episode reward: {total_reward}\")\n",
        "\n",
        "# === Основной цикл обучения ===\n",
        "def train_trpo():\n",
        "    env_name = 'CartPole-v1'\n",
        "    env = gym.make(env_name, render_mode=\"human\")\n",
        "    eval_env = gym.make(env_name, render_mode=\"human\")\n",
        "    num_inputs = env.observation_space.shape[0]\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    model = ActorCritic(num_inputs, num_actions).to(device)\n",
        "    model.critic_optimizer = optim.Adam(model.critic.parameters())\n",
        "\n",
        "    for iteration in range(100):\n",
        "        states, actions, rewards, returns, old_log_probs, masks, avg_reward = collect_trajectories(env, model)\n",
        "\n",
        "        advantages = returns - model.critic(states).squeeze()\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        trpo_update(model, states, actions, old_log_probs, advantages, max_kl=0.01)\n",
        "\n",
        "        print(f\"Iteration {iteration}, Avg Reward: {avg_reward:.2f}\")\n",
        "\n",
        "        if iteration % 10 == 0:\n",
        "            test_model(eval_env, model)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_trpo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_W_KSJO8dMz",
        "outputId": "9d8c6a0e-efe0-4877-bfbe-b6b0feb79d91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Avg Reward: 10.00\n",
            "Test episode reward: 24.0\n",
            "Test episode reward: 13.0\n",
            "Test episode reward: 14.0\n",
            "Iteration 1, Avg Reward: 16.00\n",
            "Iteration 2, Avg Reward: 16.00\n",
            "Iteration 3, Avg Reward: 23.00\n",
            "Iteration 4, Avg Reward: 15.00\n",
            "Iteration 5, Avg Reward: 16.00\n",
            "Iteration 6, Avg Reward: 10.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 7, Avg Reward: 24.00\n",
            "Iteration 8, Avg Reward: 49.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 9, Avg Reward: 14.00\n",
            "Iteration 10, Avg Reward: 11.00\n",
            "Test episode reward: 15.0\n",
            "Test episode reward: 16.0\n",
            "Test episode reward: 40.0\n",
            "Iteration 11, Avg Reward: 17.00\n",
            "Iteration 12, Avg Reward: 44.00\n",
            "Iteration 13, Avg Reward: 14.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 14, Avg Reward: 42.00\n",
            "Iteration 15, Avg Reward: 27.00\n",
            "Iteration 16, Avg Reward: 20.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 17, Avg Reward: 19.00\n",
            "Iteration 18, Avg Reward: 39.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 19, Avg Reward: 17.00\n",
            "Iteration 20, Avg Reward: 38.00\n",
            "Test episode reward: 10.0\n",
            "Test episode reward: 35.0\n",
            "Test episode reward: 78.0\n",
            "Iteration 21, Avg Reward: 18.00\n",
            "Iteration 22, Avg Reward: 21.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 23, Avg Reward: 22.00\n",
            "Iteration 24, Avg Reward: 35.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 25, Avg Reward: 19.00\n",
            "Iteration 26, Avg Reward: 33.00\n",
            "Iteration 27, Avg Reward: 81.00\n",
            "Iteration 28, Avg Reward: 64.00\n",
            "Iteration 29, Avg Reward: 28.00\n",
            "Iteration 30, Avg Reward: 115.00\n",
            "Test episode reward: 98.0\n",
            "Test episode reward: 15.0\n",
            "Test episode reward: 82.0\n",
            "Iteration 31, Avg Reward: 34.00\n",
            "Iteration 32, Avg Reward: 52.00\n",
            "Iteration 33, Avg Reward: 47.00\n",
            "Iteration 34, Avg Reward: 16.00\n",
            "Iteration 35, Avg Reward: 70.00\n",
            "Iteration 36, Avg Reward: 42.00\n",
            "Iteration 37, Avg Reward: 35.00\n",
            "Iteration 38, Avg Reward: 80.00\n",
            "Iteration 39, Avg Reward: 26.00\n",
            "Iteration 40, Avg Reward: 66.00\n",
            "Test episode reward: 81.0\n",
            "Test episode reward: 70.0\n",
            "Test episode reward: 16.0\n",
            "Iteration 41, Avg Reward: 34.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 42, Avg Reward: 105.00\n",
            "Iteration 43, Avg Reward: 23.00\n",
            "Iteration 44, Avg Reward: 70.00\n",
            "Iteration 45, Avg Reward: 73.00\n",
            "Iteration 46, Avg Reward: 57.00\n",
            "Iteration 47, Avg Reward: 62.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 48, Avg Reward: 29.00\n",
            "Iteration 49, Avg Reward: 58.00\n",
            "Iteration 50, Avg Reward: 108.00\n",
            "Test episode reward: 103.0\n",
            "Test episode reward: 35.0\n",
            "Test episode reward: 34.0\n",
            "Iteration 51, Avg Reward: 15.00\n",
            "Iteration 52, Avg Reward: 23.00\n",
            "Iteration 53, Avg Reward: 45.00\n",
            "Iteration 54, Avg Reward: 109.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 55, Avg Reward: 45.00\n",
            "Iteration 56, Avg Reward: 228.00\n",
            "Iteration 57, Avg Reward: 39.00\n",
            "Iteration 58, Avg Reward: 14.00\n",
            "Iteration 59, Avg Reward: 25.00\n",
            "Iteration 60, Avg Reward: 39.00\n",
            "Test episode reward: 61.0\n",
            "Test episode reward: 38.0\n",
            "Test episode reward: 74.0\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 61, Avg Reward: 50.00\n",
            "Iteration 62, Avg Reward: 66.00\n",
            "Iteration 63, Avg Reward: 62.00\n",
            "Iteration 64, Avg Reward: 72.00\n",
            "Iteration 65, Avg Reward: 50.00\n",
            "Iteration 66, Avg Reward: 84.00\n",
            "Iteration 67, Avg Reward: 45.00\n",
            "Iteration 68, Avg Reward: 25.00\n",
            "Iteration 69, Avg Reward: 20.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 70, Avg Reward: 37.00\n",
            "Test episode reward: 49.0\n",
            "Test episode reward: 52.0\n",
            "Test episode reward: 43.0\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 71, Avg Reward: 35.00\n",
            "Iteration 72, Avg Reward: 28.00\n",
            "Iteration 73, Avg Reward: 33.00\n",
            "Iteration 74, Avg Reward: 24.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 75, Avg Reward: 34.00\n",
            "Iteration 76, Avg Reward: 109.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 77, Avg Reward: 73.00\n",
            "NaN detected in logits. Skipping update.\n",
            "Iteration 78, Avg Reward: 106.00\n",
            "Iteration 79, Avg Reward: 100.00\n",
            "Iteration 80, Avg Reward: 81.00\n",
            "Test episode reward: 30.0\n",
            "Test episode reward: 17.0\n",
            "Test episode reward: 16.0\n",
            "Iteration 81, Avg Reward: 19.00\n",
            "Iteration 82, Avg Reward: 85.00\n",
            "Iteration 83, Avg Reward: 49.00\n",
            "Iteration 84, Avg Reward: 155.00\n",
            "Iteration 85, Avg Reward: 19.00\n",
            "Iteration 86, Avg Reward: 74.00\n",
            "Iteration 87, Avg Reward: 62.00\n",
            "Iteration 88, Avg Reward: 123.00\n",
            "Iteration 89, Avg Reward: 44.00\n",
            "Iteration 90, Avg Reward: 36.00\n",
            "Test episode reward: 48.0\n",
            "Test episode reward: 24.0\n",
            "Test episode reward: 170.0\n",
            "Iteration 91, Avg Reward: 19.00\n",
            "Iteration 92, Avg Reward: 18.00\n",
            "Iteration 93, Avg Reward: 62.00\n",
            "Iteration 94, Avg Reward: 65.00\n",
            "Iteration 95, Avg Reward: 22.00\n",
            "Iteration 96, Avg Reward: 37.00\n",
            "Iteration 97, Avg Reward: 33.00\n",
            "Iteration 98, Avg Reward: 43.00\n",
            "Iteration 99, Avg Reward: 54.00\n"
          ]
        }
      ]
    }
  ]
}